Testing and Evaluation (T&E) Framework
======================================


Usefulness, Usability, and Efficacy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*(additional detail for the Responsible AI Principle of Usefulness,
Usability, and Efficacy can be found in the CHAI RAIG)*

**Recommended Methods/Metrics:**

- **Evidence Concordance, Citation Accuracy, and Guideline Alignment**

  - Intended Use: Ensures the clinical validity and trustworthiness of AI-supported CDS recommendations, confirming they adhere to accepted medical guidelines and evidence hierarchies.
  - Description: Measures how well CDS outputs align with specialty guidelines and reference evidence while ensuring that all cited sources are verifiable and correctly attributed. Evaluates factual alignment, reference traceability, and the percentage of outputs meeting specialty-specific evidence standards.
  - Pre-implementation, Post-Implementation, or Both: Pre-implementation
  - Persona (Developer, Implementer, or Both): Both
  - Benchmark: ≥ 90% evidence concordance with specialty guidelines, 100% verifiable citations, <5% hallucinations in reference-linked outputs.
  - Supporting Literature: `Frontiers in Oncology (2023) <https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1224347/full>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `LangChain Evaluation Criteria <https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain_classic/evaluation/criteria>`__
  - Notes about Open-source Tooling: See `langchain/evaluation/criteria.py` for built-in evaluation criteria (e.g., `criteria="faithfulness"`, `criteria="relevance"`) to quantify evidence concordance and citation correctness in CDS or generative AI outputs.
  - Evaulation Example: See Citation_Accuracy.ipynb and Evidence_Concordance_Eval.ipynb in evaulation-notebooks folder.


- **DRP Detection Accuracy**

  - Intended Use: Validates the clinical safety and medication-management reliability of AI-generated CDS outputs by ensuring accurate identification of Drug-Related Problems (DRPs) during real-time decision support.
  - Description: Measures the ability of CDS models to detect and classify DRPs such as adverse drug interactions, contraindications, or incorrect dosages in medication guidance tasks. Evaluates precision, recall, and F1-score relative to pharmacist-verified ground truth.
  - Pre-implementation, Post-Implementation, or Both: Both
  - Persona (Developer, Implementer, or Both): Both
  - Benchmark: RAG-LLM + pharmacist workflow achieved higher F1 and accuracy than LLM alone across 12 specialties, improving severe DRP identification with minimal precision trade-off.
  - Supporting Literature: Ong et al., *Development and Testing of a Novel LLM-Based CDSS for Medication Safety in 12 Specialties (2024)*
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `Great Expectations Core <https://github.com/great-expectations/great_expectations/tree/develop/great_expectations/expectations/core>`__
  - Notes about Open-source Tooling: See `great_expectations/expectations/core/ColumnPairMapExpectation` for defining safety rules that verify valid drug–drug or drug–condition combinations in extracted CDS outputs.


- **Hallucination Rate and Response Time**

  - Intended Use: Confirms operational readiness and efficiency of CDS systems in clinical workflows by ensuring low hallucination rates and acceptable generation latency relative to human review.
  - Description: Measures the factual reliability and latency of CDS responses generated by LLMs. Quantifies hallucination frequency in clinical summaries or recommendations and the average time to produce accurate, guideline-consistent outputs.
  - Pre-implementation, Post-Implementation, or Both: Both
  - Persona (Developer, Implementer, or Both): Both
  - Benchmark: GPT-4 RAG achieved 96.4% accuracy vs. 86.6% for clinicians, 0% hallucinations, and ~20 s average output time compared to ~10 min human review.
  - Supporting Literature: `medRxiv (2025) <https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1.full?>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `LangChain OpenEvals <https://github.com/langchain-ai/openevals>`__
  - Notes about Open-source Tooling: See `langchain-ai/openevals` in the OpenEvals repository for frameworks to define, run, and evaluate hallucination-rate and response-time experiments. Use built-in metrics like `eval_time` for latency and `failless_rate` or custom reward functions to quantify hallucinations in LLM-driven CDS tasks.

- **Technology Acceptance Model (TAM) Survey**

  - Intended Use: Assesses perceived usability and acceptance of AI tools in clinical workflows. Supports evaluation of AI system adoption and trust, transferable to CDS tools used in triage, risk alerts, or chronic disease management.
  - Description: Captures clinician-reported usability and ease of use of a preventive care CDS tool after interaction with simulated patient charts.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation (simulated chart testing)
  - Persona (Developer, Implementer, or Both): Implementer — used after deployment to evaluate clinician acceptance and usability of AI tools in practice.
  - Benchmark: Median scores = 7/7 for both usefulness and usability
  - Supporting Literature: `BMC Med Inform Decis Mak (2021) <https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01675-8>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: Any surveying tool can be used for administration and scoring.
  - Notes about Open-source Tooling: Survey instruments based on the Technology Acceptance Model (TAM) framework are recommended for standardized clinician usability and acceptance evaluation.


- **Time to complete medication reviews (in minutes) using clickstream data and linear mixed models**

  - Intended Use: Measures real-world efficiency gains from AI tool adoption after deployment by evaluating whether clinicians performing decision-supported medication reviews become faster with continued use.
  - Description: This study assessed whether physicians and pharmacists using the STRIP Assistant (a CDSS web tool) completed medication reviews more efficiently over time. It used clickstream data and linear mixed models in a longitudinal analysis within a randomized controlled trial in the Netherlands.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation
  - Persona (Developer, Implementer, or Both): Implementer — measures how familiarity and experience with the CDSS impact task completion time and efficiency.
  - Benchmark: Early median review time = 20.4 min (SD 9.0); later median review time = 9.8 min (SD 6.1); F(31.145)=14.043, p<.001
  - Supporting Literature: `PMC4720692 <https://pmc.ncbi.nlm.nih.gov/articles/PMC4720692/>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `Matomo <https://matomo.org/>`__ (tracks session data, timestamps, and user activity); `Snowplow <https://snowplow.io/>`__ (not fully open-source but usable for event-level tracking)
  - Notes about Open-source Tooling: Any clickstream analytics or telemetry framework capable of capturing session duration and user interactions can operationalize this metric for AI usability assessment.

- **Pajama Time**

  - Intended Use: Evaluates the impact of deployed AI tools on clinician workload and after-hours documentation time, supporting assessments of work-life balance and operational efficiency.
  - Description: Measures time spent in the EHR outside of scheduled clinic hours (“pajama time”). Used to quantify documentation burden following AI-supported CDS adoption in clinical workflows, especially in longitudinal care or complex case reviews where delayed documentation is common.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation
  - Persona (Developer, Implementer, or Both): Implementer — assesses whether AI documentation or CDS systems reduce after-hours work following deployment.
  - Benchmark: Reduction from ≥90 to <30 minutes per day (Mass General Brigham); 48–60% drop reported in pilot studies (Abridge)
  - Supporting Literature: `PHTI Adoption of AI in Healthcare Delivery Systems (2025) <https://phti.org/wp-content/uploads/sites/3/2025/03/PHTI-Adoption-of-AI-in-Healthcare-Delivery-Systems-Early-Applications-Impacts.pdf>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: Not a direct open-source tool, but metric calculations have been demonstrated in published EHR workload analyses such as `PMC6712097 <https://pmc.ncbi.nlm.nih.gov/articles/PMC6712097/>`__
  - Notes about Open-source Tooling: EHR audit logs or time-tracking APIs (e.g., Epic Signal or Cerner Lights On) can be leveraged to operationalize pajama time measurement within CDS evaluation frameworks.

**Cognitive Load**

  - Intended Use: To evaluate how AI tools reduce the mental burden of clinical documentation. Supports CDS systems aiming to streamline information processing and reduce fatigue in cognitively demanding workflows like guideline review or complex case synthesis.
  - Description: Measured reduction in mental effort required for documentation using ambient scribe, based on self-reported feedback and note retention rates.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation
  - Persona (Developer, Implementer, or Both): Implementer — assesses real-world reduction in mental effort after AI tool deployment.
  - Benchmark: 61% decrease in cognitive load (Corewell); 80% of AI-generated note retained (Yale)
  - Supporting Literature: `PHTI Adoption of AI in Healthcare Delivery Systems (2025) <https://phti.org/wp-content/uploads/sites/3/2025/03/PHTI-Adoption-of-AI-in-Healthcare-Delivery-Systems-Early-Applications-Impacts.pdf>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `TimDomino/nasa-tlx <https://github.com/TimDomino/nasa-tlx/blob/master/nasa-tlx.py>`__, `jmpolom/NASA-TLX <https://github.com/jmpolom/NASA-TLX/blob/master/src/main.py>`__
  - Notes about Open-source Tooling: To measure clinical cognitive load, companies can deploy TLX surveys using the TimDomino/nasa-tlx script for data collection and use jmpolom/NASA-TLX for computing weighted workload scores. This enables pre- vs post-deployment comparisons of mental effort in documentation workflows.

**Burnout (Mini Z Survey)**

  - Intended Use: To validate whether AI documentation support reduces clinician strain in high-volume settings. Supports CDS tools that aim to alleviate cognitive and emotional workload in time-pressured clinical workflows like preventive care or triage.
  - Description: Survey-based measurement of clinician burnout following ambient scribe adoption.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation
  - Persona (Developer, Implementer, or Both): Implementer — measures clinician well-being improvements after AI tool integration in practice.
  - Benchmark: 40% reduction in burnout (Mass General Brigham); 63% reduction (MultiCare)
  - Supporting Literature: `PHTI Adoption of AI in Healthcare Delivery Systems (2025) <https://phti.org/wp-content/uploads/sites/3/2025/03/PHTI-Adoption-of-AI-in-Healthcare-Delivery-Systems-Early-Applications-Impacts.pdf>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: N/A
  - Notes about Open-source Tooling: The Mini Z burnout survey is a validated, standardized instrument that can be adapted for AI-CDS evaluation to track clinician wellness outcomes pre- and post-deployment.

Fairness and Bias Management
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Stratified Subgroup Performance + eXplainable AI (XAI)**

  - Intended Use: To systematically detect and explain performance disparities across protected groups before deployment. Subgroup error analysis combined with XAI enables corrective interventions, and this method can be replicated in any CDS system to preemptively flag biased model behavior.
  - Description: FairLens audits black-box CDS using subgroup error rates across age, gender, ethnicity, and insurance, then provides explainable insights into bias patterns.
  - Pre-implementation, Post-Implementation, or Both: Pre-implementation
  - Persona (Developer, Implementer, or Both): Developer — used before deployment to audit and explain subgroup bias in CDS model behavior.
  - Benchmark: Stratified audit feasibility demonstrated; no explicit numeric thresholds, but the tool identifies biased subgroups for expert review.
  - Supporting Literature: `PubMed 37695082 <https://pubmed.ncbi.nlm.nih.gov/37695082/>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `Fairlearn MetricFrame <https://github.com/fairlearn/fairlearn/blob/main/fairlearn/metrics/_metric_frame.py>`__, `InterpretML SHAP Notebook <https://github.com/interpretml/interpret/blob/main/docs/interpret/shap.ipynb>`__
  - Notes about Open-source Tooling: Combine Fairlearn’s MetricFrame to compute subgroup error rates with InterpretML’s SHAP/LIME explainers to identify feature-level drivers of bias. Together, these open-source tools replicate the FairLens workflow for detecting subgroup disparities and explaining their root causes before deployment.

**True Positive Rate (TPR) Disparity Analysis**

  - Intended Use: TPR disparity is used to quantify fairness in diagnostic models. Measuring subgroup-specific TPR guides training interventions such as data balancing or re-weighting. This metric applies to any image-based CDS to ensure equitable sensitivity across demographics.
  - Description: CheXclusion evaluated deep chest X-ray classifiers across age, race, sex, and insurance; measured differences in TPR to assess bias.
  - Pre-implementation, Post-Implementation, or Both: Pre-implementation
  - Persona (Developer, Implementer, or Both): Developer — measures and mitigates fairness gaps in model training before deployment.
  - Benchmark: Demonstrated significant TPR disparities in all datasets; multi-source training reduced but didn’t eliminate gaps.
  - Supporting Literature: `PubMed 33691020 <https://pubmed.ncbi.nlm.nih.gov/33691020/>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `Fairlearn TPR Metric <https://fairlearn.org/v0.12/api_reference/generated/fairlearn.metrics.true_positive_rate.html>`__
  - Notes about Open-source Tooling: Use Fairlearn’s `true_positive_rate` metric to calculate subgroup-specific sensitivity across protected attributes, enabling quantification and monitoring of fairness disparities prior to model deployment.


**Subgroup QA Accuracy across Sensitive Attributes**

  - Intended Use: Subgroup-based accuracy combined with prompt and bias sensitivity offers a template for evaluating fairness in LLM-powered CDS. It highlights which prompting styles and models minimize bias and can be used to tune clinical LLM outputs in any decision-support context.
  - Description: Evaluated eight LLMs on three clinical QA vignettes varying race, age, and gender; compared accuracy and language use across subgroups and prompt designs.
  - Pre-implementation, Post-Implementation, or Both: Pre-implementation
  - Persona (Developer, Implementer, or Both): Developer — used to evaluate and reduce subgroup bias in LLM outputs before deployment.
  - Benchmark: Significant bias variations across models and prompts; chain-of-thought prompting reduced subgroup disparities.
  - Supporting Literature: `arXiv:2404.15149 <https://arxiv.org/pdf/2404.15149>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: `Fairlearn MetricFrame <https://github.com/fairlearn/fairlearn/blob/main/fairlearn/metrics/_metric_frame.py>`__
  - Notes about Open-source Tooling: Use Fairlearn’s MetricFrame to compute subgroup-specific QA accuracy across sensitive attributes, directly replicating fairness evaluations from clinical vignette studies. By pairing this with different prompting strategies, identify which LLM configurations minimize subgroup disparities before deployment.





Safety and Reliability
~~~~~~~~~~~~~~~~~~~~~~

**Recommended Methods/Metrics:**

**Multidimensional Human-in-the-Loop Evaluation (Helpfulness, Comprehension, Correctness, Completeness, Clinical Harmfulness)**

  - Intended Use: This multidimensional framework is intended to bridge the gap between purely automated evaluation (e.g., BLEU or ROUGE) and human-centered, safety-critical evaluation of medical AI systems. The metrics target the most pressing questions for deploying generative AI in health: Is it helpful to the clinician? Does it correctly understand medical questions? Is the information factually accurate? Does it fully answer the question? Could it harm patients if blindly followed? These dimensions are highly transferable to other AI systems beyond question-answering—such as AI-generated discharge summaries, educational materials, or radiology reports—since any clinical AI output must address (1) usability and perceived helpfulness, (2) correct comprehension of medical context, (3) factual accuracy, (4) comprehensiveness, and (5) potential to cause harm. The rigorous SME-in-the-loop, consensus-driven scoring and reproducible process is particularly valuable for standard-setting in regulated environments where patient safety and clinical validity cannot rely solely on automated metrics. The methodology supports scalable yet clinically grounded oversight of generative AI tools in other specialties, languages, or public-facing consumer health applications.
  - Description: This study developed and demonstrated a reproducible framework for evaluating generative AI in health care. The framework was applied to ClinicalKey AI, a retrieval-augmented generation (RAG) tool, across 426 query-response pairs from real user logs, a benchmark dataset, and SME-created queries. Board-certified clinicians scored responses on helpfulness, query comprehension, factual correctness, completeness, and potential harm, using multi-reviewer consensus and tie-breaker methods.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation (evaluating a production GenAI system’s outputs in real-world-like scenarios, though its generalizability means it could also support pre-deployment validation)
  - Persona (Developer, Implementer, or Both): Both — used by developers for pre-deployment validation and by implementers for post-deployment safety and quality monitoring.
  - Benchmark: 94.4% of responses rated helpful; 95.5% factually correct; 98.6% query comprehension; 90.9% completeness; 0.47% potentially harmful responses (with detailed severity breakdowns); agreement between first two SMEs in 60.6% of evaluations.
  - Supporting Literature: `Elsevier / JAMIA Open (2025) <https://academic.oup.com/jamiaopen/article/8/3/ooaf054/8163901?login=false>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: Framework referenced in Elsevier’s ClinicalKey AI evaluation; see Appendix materials within the publication for rubric templates, SME evaluation protocol, and scoring methodology.
  - Notes about Open-source Tooling: Reference the full Elsevier evaluation framework (Appendix and Supplementary Methods) for detailed scoring rubrics, consensus processes, and SME agreement calculations. This structure parallels the QUEST framework used in clinical trials T&E for reproducibility and cross-domain generalizability.

**Multistakeholder Requirements Analysis**

  - Intended Use: To ensure CDS tools meet the operational and safety needs of real-world clinical environments before deployment. Captures design requirements—such as recommendation applicability and raw data access—that are essential for building systems that not only surface guidance but can act on it reliably. Transferable to other guideline-based domains (e.g., oncology, cardiology) that require dynamic integration of evolving recommendations into patient-level decision logic.
  - Description: Focus groups with clinicians, engineers, and developers to identify core system needs for guideline adherence monitoring.
  - Pre-implementation, Post-Implementation, or Both: Pre-implementation (requirements capture)
  - Persona (Developer, Implementer, or Both): Developer — captures design requirements before system development to ensure safe and reliable CDS functionality.
  - Benchmark: Identified four functional requirements: recommendation applicability, data integration, raw data access, and FHIR-based interoperability.
  - Supporting Literature: `JMIR (2023) <https://www.jmir.org/2023/1/e41177>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: N/A
  - Notes about Open-source Tooling: Methods can be adapted using qualitative research frameworks (e.g., focus group analysis in NVivo or ATLAS.ti) to replicate multistakeholder requirement capture for CDS or other safety-critical AI systems.

**Prototype Testing with Real Patient Data**

  - Intended Use: To validate whether AI systems can go beyond passive display of guidelines and actively assess their relevance and implementation at the point of care. Demonstrates how guideline-driven CDS can become executable and personalized when tightly linked to structured EHR data. This type of evaluation supports broader application to any specialty where care quality depends on real-time guideline adherence, such as sepsis management, stroke protocols, or cancer staging workflows.
  - Description: Evaluation of a modular guideline adherence system using real COVID-19 patient records to verify decision-support performance.
  - Pre-implementation, Post-Implementation, or Both: Post-implementation (prototype evaluation with patient data)
  - Persona (Developer, Implementer, or Both): Implementer — tests system performance using real patient data to ensure reliable decision support after deployment.
  - Benchmark: Demonstrated feasibility of real-time guideline adherence checks; clinical outcome impact pending future studies.
  - Supporting Literature: `JMIR (2023) <https://www.jmir.org/2023/1/e41177>`__
  - Alignment to Use Case (Low, Medium, High): High
  - Open-source Tooling: N/A
  - Notes about Open-source Tooling: Prototype testing methods can be replicated using synthetic or de-identified patient datasets and open-source FHIR servers (e.g., HAPI FHIR) to validate guideline adherence logic before clinical implementation.

